{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we are importing the spark context module from the pyspark library\n",
    "from pyspark import SparkContext\n",
    "# we are intializing sparkcontext and setting it to sc\n",
    "sc = SparkContext()\n",
    "#creating a list of numbers assigned to the variable\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#passing list to parallelize method, lazy, has some id of the data but hasnt done anything yet\n",
    "#parrelize just sends it to the dag aka to the driver.  once we do an action the rdd is sent to the cluster to be computed\n",
    "rdds = sc.parallelize(data)\n",
    "#collect action triggers the transformations in the dag\n",
    "display(rdds, rdds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rdds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rdds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,\"Goroka\",\"Goroka\",\"Papua New Guinea\",\"GKA\",\"AYGA\",-6.081689,145.391881,5282,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '2,\"Madang\",\"Madang\",\"Papua New Guinea\",\"MAG\",\"AYMD\",-5.207083,145.7887,20,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '3,\"Mount Hagen\",\"Mount Hagen\",\"Papua New Guinea\",\"HGU\",\"AYMH\",-5.826789,144.295861,5388,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '4,\"Nadzab\",\"Nadzab\",\"Papua New Guinea\",\"LAE\",\"AYNZ\",-6.569828,146.726242,239,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '5,\"Port Moresby Jacksons Intl\",\"Port Moresby\",\"Papua New Guinea\",\"POM\",\"AYPY\",-9.443383,147.22005,146,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '6,\"Wewak Intl\",\"Wewak\",\"Papua New Guinea\",\"WWK\",\"AYWK\",-3.583828,143.669186,19,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '7,\"Narsarsuaq\",\"Narssarssuaq\",\"Greenland\",\"UAK\",\"BGBW\",61.160517,-45.425978,112,-3,\"E\",\"America/Godthab\"',\n",
       " '8,\"Nuuk\",\"Godthaab\",\"Greenland\",\"GOH\",\"BGGH\",64.190922,-51.678064,283,-3,\"E\",\"America/Godthab\"',\n",
       " '9,\"Sondre Stromfjord\",\"Sondrestrom\",\"Greenland\",\"SFJ\",\"BGSF\",67.016969,-50.689325,165,-3,\"E\",\"America/Godthab\"']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "#since getorcreate it won't throw an error if a context was already created\n",
    "#sc is more a workplace to manipulate the data for the next process than what we'd considered a standard python object\n",
    "sc = SparkContext.getOrCreate()\n",
    "# remember to change the path/location of file\n",
    "airports = sc.textFile(\"airports.txt\")\n",
    "#same as head, take is an action\n",
    "airports.take(9)\n",
    "#.take() : shows content and structure/metadata for a limited number of rows for a very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PSNY9ZH3Z126.mynetworksettings.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter RDD -> ['260,\"Nnamdi Azikiwe Intl\",\"Abuja\",\"Nigeria\",\"ABV\",\"DNAA\",9.006792,7.263172,1123,1,\"N\",\"Africa/Lagos\"', '261,\"Akure\",\"Akure\",\"Nigeria\",\"AKR\",\"DNAK\",7.246739,5.301008,1100,1,\"N\",\"Africa/Lagos\"', '262,\"Benin\",\"Benin\",\"Nigeria\",\"BNI\",\"DNBE\",6.316981,5.599503,258,1,\"N\",\"Africa/Lagos\"', '263,\"Calabar\",\"Calabar\",\"Nigeria\",\"CBQ\",\"DNCA\",4.976019,8.347197,210,1,\"N\",\"Africa/Lagos\"', '264,\"Enugu\",\"Enugu\",\"Nigeria\",\"ENU\",\"DNEN\",6.474272,7.561961,466,1,\"N\",\"Africa/Lagos\"', '265,\"Gusau\",\"Gusau\",\"Nigeria\",\"QUS\",\"DNGU\",12.171667,6.696111,1520,1,\"N\",\"Africa/Lagos\"', '266,\"Ibadan\",\"Ibadan\",\"Nigeria\",\"IBA\",\"DNIB\",7.362458,3.978333,725,1,\"N\",\"Africa/Lagos\"', '267,\"Ilorin\",\"Ilorin\",\"Nigeria\",\"ILR\",\"DNIL\",8.440211,4.493919,1126,1,\"N\",\"Africa/Lagos\"', '268,\"Yakubu Gowon\",\"Jos\",\"Nigeria\",\"JOS\",\"DNJO\",9.639828,8.86905,4232,1,\"N\",\"Africa/Lagos\"', '269,\"Kaduna\",\"Kaduna\",\"Nigeria\",\"KAD\",\"DNKA\",10.696025,7.320114,2073,1,\"N\",\"Africa/Lagos\"', '270,\"Mallam Aminu Intl\",\"Kano\",\"Nigeria\",\"KAN\",\"DNKN\",12.047589,8.524622,1562,1,\"N\",\"Africa/Lagos\"', '271,\"Maiduguri\",\"Maiduguri\",\"Nigeria\",\"MIU\",\"DNMA\",11.855347,13.08095,1099,1,\"N\",\"Africa/Lagos\"', '272,\"Makurdi\",\"Makurdi\",\"Nigeria\",\"MDI\",\"DNMK\",7.703883,8.613939,371,1,\"N\",\"Africa/Lagos\"', '273,\"Murtala Muhammed\",\"Lagos\",\"Nigeria\",\"LOS\",\"DNMM\",6.577369,3.321156,135,1,\"N\",\"Africa/Lagos\"', '274,\"Minna New\",\"Minna\",\"Nigeria\",\"MXJ\",\"DNMN\",9.652172,6.462256,834,1,\"N\",\"Africa/Lagos\"', '275,\"Port Harcourt Intl\",\"Port Hartcourt\",\"Nigeria\",\"PHC\",\"DNPO\",5.015494,6.949594,87,1,\"N\",\"Africa/Lagos\"', '276,\"Sadiq Abubakar Iii Intl\",\"Sokoto\",\"Nigeria\",\"SKO\",\"DNSO\",12.916322,5.207189,1010,1,\"N\",\"Africa/Lagos\"', '277,\"Yola\",\"Yola\",\"Nigeria\",\"YOL\",\"DNYO\",9.257553,12.430422,599,1,\"N\",\"Africa/Lagos\"', '278,\"Zaria\",\"Zaria\",\"Nigeria\",\"ZAR\",\"DNZA\",11.130192,7.685806,2170,1,\"N\",\"Africa/Lagos\"', '6730,\"Imo Airport\",\"Imo\",\"Nigeria\",\"QOW\",\"DNIM\",5.42706,7.20603,373,1,\"N\",\"Africa/Lagos\"', '6972,\"Warri Airport\",\"Osubi\",\"Nigeria\",\"QRW\",\"DNSU\",5.31,5.45,50,1,\"U\",\"Africa/Lagos\"', '7439,\"Gombe Lawanti International Airport\",\"Gombe\",\"Nigeria\",\"\",\\\\N,9.2575,12.430278,500,1,\"U\",\"Africa/Lagos\"', '7440,\"Akwa Ibom International Airport\",\"Uyo\",\"Nigeria\",\"\",\\\\N,5.05,7.933333,500,1,\"U\",\"Africa/Lagos\"', '7441,\"Katsina Airport\",\"Katsina\",\"Nigeria\",\"\",\\\\N,13.007778,7.660278,50,1,\"U\",\"Africa/Lagos\"', '9022,\"Lagos Lagoon\",\"Lekki\",\"Nigeria\",\"\",\\\\N,6.465907,3.532115,0,1,\"N\",\"Africa/Lagos\"', '9023,\"Escravos\",\"Escravos\",\"Nigeria\",\"\",\\\\N,5.617203,5.187993,0,1,\"U\",\"Africa/Lagos\"']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate ()\n",
    "airports = sc.textFile(\"airports.txt\")\n",
    "#filters the txt splitting it at the commas then checking the 1st index for ones matching Putnam County Airport\n",
    "#the lambda function that is establishing the condition to be evaluated\n",
    "us_airports = airports.filter (lambda line: \"Nigeria\" in line.split(\",\")[3])\n",
    "filtered = us_airports.collect()\n",
    "print (\"Filter RDD -> %s\" % (filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate ()\n",
    "data = [\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\",\"spark vs hadoop\",\"pyspark\",\"Java\"]\n",
    "rdd=sc.parallelize(data)\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "# for element in rdd2.collect():\n",
    "#     print(element)\n",
    "type(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for element in rdd2.collect():\n",
    "    print(type(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'history', 'of', 'New', 'York', 'begins', 'around', '10,000', 'BC,', 'when', 'the', 'first', 'Native', 'Americans', 'arrived.', 'By', '1100', 'AD,', 'New', \"York's\", 'main', 'native', 'cultures,', 'the', 'Iroquoian', 'and', 'Algonquian,', 'had', 'developed.', 'European', 'discovery', 'of', 'New', 'York', 'was', 'led', 'by', 'the', 'French', 'in', '1524', 'and', 'the', 'first', 'land', 'claim', 'came', 'in', '1609', 'by', 'the', 'Dutch.', 'As', 'part', 'of', 'New', 'Netherland,', 'the', 'colony', 'was', 'important', 'in', 'the', 'fur', 'trade', 'and', 'eventually', 'became', 'an', 'agricultural', 'resource', 'thanks', 'to', 'the', 'patroon', 'system.', 'In', '1626', 'the', 'Dutch', 'bought', 'the', 'island', 'of', 'Manhattan', 'from', 'Native', 'Americans.[1]', 'In', '1664,', 'England', 'renamed', 'the', 'colony', 'New', 'York,', 'after', 'the', 'Duke', 'of', 'York', '(later', 'James', 'II', '&', 'VII.)', 'New', 'York', 'City', 'gained', 'prominence', 'in', 'the', '18th', 'century', 'as', 'a', 'major', 'trading', 'port', 'in', 'the', 'Thirteen', 'Colonies.'], [''], ['New', 'York', 'played', 'a', 'pivotal', 'role', 'during', 'the', 'American', 'Revolution', 'and', 'subsequent', 'war.', 'The', 'Stamp', 'Act', 'Congress', 'in', '1765', 'brought', 'together', 'representatives', 'from', 'across', 'the', 'Thirteen', 'Colonies', 'to', 'form', 'a', 'unified', 'response', 'to', 'British', 'policies.', 'The', 'Sons', 'of', 'Liberty', 'were', 'active', 'in', 'New', 'York', 'City', 'to', 'challenge', 'British', 'authority.', 'After', 'a', 'major', 'loss', 'at', 'the', 'Battle', 'of', 'Long', 'Island,', 'the', 'Continental', 'Army', 'suffered', 'a', 'series', 'of', 'additional', 'defeats', 'that', 'forced', 'a', 'retreat', 'from', 'the', 'New', 'York', 'City', 'area,', 'leaving', 'the', 'strategic', 'port', 'and', 'harbor', 'to', 'the', 'British', 'army', 'and', 'navy', 'as', 'their', 'North', 'American', 'base', 'of', 'operations', 'for', 'the', 'rest', 'of', 'the', 'war.', 'The', 'Battle', 'of', 'Saratoga', 'was', 'the', 'turning', 'point', 'of', 'the', 'war', 'in', 'favor', 'of', 'the', 'Americans,', 'convincing', 'France', 'to', 'formally', 'ally', 'with', 'them.', 'New', \"York's\", 'constitution', 'was', 'adopted', 'in', '1777,', 'and', 'strongly', 'influenced', 'the', 'United', 'States', 'Constitution.', 'New', 'York', 'City', 'was', 'the', 'national', 'capital', 'at', 'various', 'times', 'between', '1785', 'and', '1790,', 'where', 'the', 'Bill', 'of', 'Rights', 'was', 'drafted.', 'Albany', 'became', 'the', 'permanent', 'state', 'capital', 'in', '1797.', 'In', '1787,', 'New', 'York', 'became', 'the', 'eleventh', 'state', 'to', 'ratify', 'the', 'United', 'States', 'Constitution.'], [''], ['New', 'York', 'hosted', 'significant', 'transportation', 'advancements', 'in', 'the', '19th', 'century,', 'including', 'the', 'first', 'steamboat', 'line', 'in', '1807,', 'the', 'Erie', 'Canal', 'in', '1825,', 'and', \"America's\", 'first', 'regularly', 'scheduled', 'rail', 'service', 'in', '1831.', 'These', 'advancements', 'led', 'to', 'the', 'expanded', 'settlement', 'of', 'western', 'New', 'York', 'and', 'trade', 'ties', 'to', 'the', 'Midwest', 'settlements', 'around', 'the', 'Great', 'Lakes.'], [''], ['Due', 'to', 'New', 'York', \"City's\", 'trade', 'ties', 'to', 'the', 'South,', 'there', 'were', 'numerous', 'southern', 'sympathizers', 'in', 'the', 'early', 'days', 'of', 'the', 'American', 'Civil', 'War', 'and', 'the', 'mayor', 'proposed', 'secession.', 'Far', 'from', 'any', 'of', 'the', 'battles,', 'New', 'York', 'ultimately', 'sent', 'the', 'most', 'men', 'and', 'money', 'to', 'support', 'the', 'Union', 'cause.', 'Thereafter,', 'the', 'state', 'helped', 'create', 'the', 'industrial', 'age', 'and', 'consequently', 'was', 'home', 'to', 'some', 'of', 'the', 'first', 'labor', 'unions.'], [''], ['During', 'the', '19th', 'century,', 'New', 'York', 'City', 'became', 'the', 'main', 'entry', 'point', 'for', 'European', 'immigrants', 'to', 'the', 'United', 'States,', 'beginning', 'with', 'a', 'wave', 'of', 'Irish', 'during', 'their', 'Great', 'Famine.', 'Millions', 'came', 'through', 'Castle', 'Clinton', 'in', 'Battery', 'Park', 'before', 'Ellis', 'Island', 'opened', 'in', '1892', 'to', 'welcome', 'millions', 'more,', 'increasingly', 'from', 'eastern', 'and', 'southern', 'Europe.', 'The', 'Statue', 'of', 'Liberty', 'opened', 'in', '1886', 'and', 'became', 'a', 'symbol', 'of', 'hope.', 'New', 'York', 'boomed', 'during', 'the', 'Roaring', 'Twenties,', 'before', 'the', 'Wall', 'Street', 'Crash', 'of', '1929,', 'and', 'skyscrapers', 'expressed', 'the', 'energy', 'of', 'the', 'city.', 'New', 'York', 'City', 'was', 'the', 'site', 'of', 'successive', 'tallest', 'buildings', 'in', 'the', 'world', 'from', '1913–74.'], [''], ['The', 'buildup', 'of', 'defense', 'industries', 'for', 'World', 'War', 'II', 'turned', 'around', 'the', \"state's\", 'economy', 'from', 'the', 'Great', 'Depression,', 'as', 'hundreds', 'of', 'thousands', 'worked', 'to', 'defeat', 'the', 'Axis', 'powers.', 'Following', 'the', 'war,', 'the', 'state', 'experienced', 'significant', 'suburbanization', 'around', 'all', 'the', 'major', 'cities,', 'and', 'most', 'central', 'cities', 'shrank.', 'The', 'Thruway', 'system', 'opened', 'in', '1956,', 'signalling', 'another', 'era', 'of', 'transportation', 'advances.'], [''], ['Following', 'a', 'period', 'of', 'near–bankruptcy', 'in', 'the', 'late', '1970s,', 'New', 'York', 'City', 'renewed', 'its', 'stature', 'as', 'a', 'cultural', 'center,', 'attracted', 'more', 'immigration,', 'and', 'hosted', 'the', 'development', 'of', 'new', 'music', 'styles.', 'The', 'city', 'developed', 'from', 'publishing', 'to', 'become', 'a', 'media', 'capital', 'over', 'the', 'second', 'half', 'of', 'the', '20th', 'century,', 'hosting', 'most', 'national', 'news', 'channels', 'and', 'broadcasts.', 'Some', 'of', 'its', 'newspapers', 'became', 'nationally', 'and', 'globallyrenowned.', 'The', \"state's\", 'manufacturing', 'base', 'eroded', 'with', 'the', 'restructuring', 'of', 'industry,', 'and', 'the', 'state', 'transitioned', 'into', 'service', 'industries.'], [''], ['The', 'September', '11', 'attacks', 'of', '2001', 'destroyed', 'the', 'World', 'Trade', 'Center,', 'killing', 'almost', '3,000', 'people;', 'they', 'were', 'the', 'largest', 'terrorist', 'attacks', 'on', 'United', 'States', 'soil.']]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate ()\n",
    "dataRDD = sc.textFile(\"word_count.txt\")\n",
    "words=dataRDD.map(lambda x: x.split(\" \"))\n",
    "print(words.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of numbers is :  677\n"
     ]
    }
   ],
   "source": [
    "sampledata = sc.parallelize([14,21,88,99,455])\n",
    "#aggregate numbers using addition operator\n",
    "sum = sampledata.reduce(lambda a,b: a+b)\n",
    "print(\"Sum of numbers is : \", sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n",
      "defaultdict(<class 'int'>, {'The': 10, 'history': 1, 'of': 33, 'New': 20, 'York': 17, 'begins': 1, 'around': 4, '10,000': 1, 'BC,': 1, 'when': 1, 'the': 71, 'first': 5, 'Native': 2, 'Americans': 1, 'arrived.': 1, 'By': 1, '1100': 1, 'AD,': 1, \"York's\": 2, 'main': 2, 'native': 1, 'cultures,': 1, 'Iroquoian': 1, 'and': 21, 'Algonquian,': 1, 'had': 1, 'developed.': 1, 'European': 2, 'discovery': 1, 'was': 8, 'led': 2, 'by': 2, 'French': 1, 'in': 21, '1524': 1, 'land': 1, 'claim': 1, 'came': 2, '1609': 1, 'Dutch.': 1, 'As': 1, 'part': 1, 'Netherland,': 1, 'colony': 2, 'important': 1, 'fur': 1, 'trade': 3, 'eventually': 1, 'became': 6, 'an': 1, 'agricultural': 1, 'resource': 1, 'thanks': 1, 'to': 17, 'patroon': 1, 'system.': 1, 'In': 3, '1626': 1, 'Dutch': 1, 'bought': 1, 'island': 1, 'Manhattan': 1, 'from': 8, 'Americans.[1]': 1, '1664,': 1, 'England': 1, 'renamed': 1, 'York,': 1, 'after': 1, 'Duke': 1, '(later': 1, 'James': 1, 'II': 2, '&': 1, 'VII.)': 1, 'City': 7, 'gained': 1, 'prominence': 1, '18th': 1, 'century': 1, 'as': 4, 'a': 11, 'major': 3, 'trading': 1, 'port': 2, 'Thirteen': 2, 'Colonies.': 1, 'played': 1, 'pivotal': 1, 'role': 1, 'during': 3, 'American': 3, 'Revolution': 1, 'subsequent': 1, 'war.': 2, 'Stamp': 1, 'Act': 1, 'Congress': 1, '1765': 1, 'brought': 1, 'together': 1, 'representatives': 1, 'across': 1, 'Colonies': 1, 'form': 1, 'unified': 1, 'response': 1, 'British': 3, 'policies.': 1, 'Sons': 1, 'Liberty': 2, 'were': 3, 'active': 1, 'challenge': 1, 'authority.': 1, 'After': 1, 'loss': 1, 'at': 2, 'Battle': 2, 'Long': 1, 'Island,': 1, 'Continental': 1, 'Army': 1, 'suffered': 1, 'series': 1, 'additional': 1, 'defeats': 1, 'that': 1, 'forced': 1, 'retreat': 1, 'area,': 1, 'leaving': 1, 'strategic': 1, 'harbor': 1, 'army': 1, 'navy': 1, 'their': 2, 'North': 1, 'base': 2, 'operations': 1, 'for': 3, 'rest': 1, 'Saratoga': 1, 'turning': 1, 'point': 2, 'war': 1, 'favor': 1, 'Americans,': 1, 'convincing': 1, 'France': 1, 'formally': 1, 'ally': 1, 'with': 3, 'them.': 1, 'constitution': 1, 'adopted': 1, '1777,': 1, 'strongly': 1, 'influenced': 1, 'United': 4, 'States': 3, 'Constitution.': 2, 'national': 2, 'capital': 3, 'various': 1, 'times': 1, 'between': 1, '1785': 1, '1790,': 1, 'where': 1, 'Bill': 1, 'Rights': 1, 'drafted.': 1, 'Albany': 1, 'permanent': 1, 'state': 5, '1797.': 1, '1787,': 1, 'eleventh': 1, 'ratify': 1, 'hosted': 2, 'significant': 2, 'transportation': 2, 'advancements': 2, '19th': 2, 'century,': 3, 'including': 1, 'steamboat': 1, 'line': 1, '1807,': 1, 'Erie': 1, 'Canal': 1, '1825,': 1, \"America's\": 1, 'regularly': 1, 'scheduled': 1, 'rail': 1, 'service': 2, '1831.': 1, 'These': 1, 'expanded': 1, 'settlement': 1, 'western': 1, 'ties': 2, 'Midwest': 1, 'settlements': 1, 'Great': 3, 'Lakes.': 1, 'Due': 1, \"City's\": 1, 'South,': 1, 'there': 1, 'numerous': 1, 'southern': 2, 'sympathizers': 1, 'early': 1, 'days': 1, 'Civil': 1, 'War': 2, 'mayor': 1, 'proposed': 1, 'secession.': 1, 'Far': 1, 'any': 1, 'battles,': 1, 'ultimately': 1, 'sent': 1, 'most': 3, 'men': 1, 'money': 1, 'support': 1, 'Union': 1, 'cause.': 1, 'Thereafter,': 1, 'helped': 1, 'create': 1, 'industrial': 1, 'age': 1, 'consequently': 1, 'home': 1, 'some': 1, 'labor': 1, 'unions.': 1, 'During': 1, 'entry': 1, 'immigrants': 1, 'States,': 1, 'beginning': 1, 'wave': 1, 'Irish': 1, 'Famine.': 1, 'Millions': 1, 'through': 1, 'Castle': 1, 'Clinton': 1, 'Battery': 1, 'Park': 1, 'before': 2, 'Ellis': 1, 'Island': 1, 'opened': 3, '1892': 1, 'welcome': 1, 'millions': 1, 'more,': 1, 'increasingly': 1, 'eastern': 1, 'Europe.': 1, 'Statue': 1, '1886': 1, 'symbol': 1, 'hope.': 1, 'boomed': 1, 'Roaring': 1, 'Twenties,': 1, 'Wall': 1, 'Street': 1, 'Crash': 1, '1929,': 1, 'skyscrapers': 1, 'expressed': 1, 'energy': 1, 'city.': 1, 'site': 1, 'successive': 1, 'tallest': 1, 'buildings': 1, 'world': 1, '1913–74.': 1, 'buildup': 1, 'defense': 1, 'industries': 1, 'World': 2, 'turned': 1, \"state's\": 2, 'economy': 1, 'Depression,': 1, 'hundreds': 1, 'thousands': 1, 'worked': 1, 'defeat': 1, 'Axis': 1, 'powers.': 1, 'Following': 2, 'war,': 1, 'experienced': 1, 'suburbanization': 1, 'all': 1, 'cities,': 1, 'central': 1, 'cities': 1, 'shrank.': 1, 'Thruway': 1, 'system': 1, '1956,': 1, 'signalling': 1, 'another': 1, 'era': 1, 'advances.': 1, 'period': 1, 'near–bankruptcy': 1, 'late': 1, '1970s,': 1, 'renewed': 1, 'its': 2, 'stature': 1, 'cultural': 1, 'center,': 1, 'attracted': 1, 'more': 1, 'immigration,': 1, 'development': 1, 'new': 1, 'music': 1, 'styles.': 1, 'city': 1, 'developed': 1, 'publishing': 1, 'become': 1, 'media': 1, 'over': 1, 'second': 1, 'half': 1, '20th': 1, 'hosting': 1, 'news': 1, 'channels': 1, 'broadcasts.': 1, 'Some': 1, 'newspapers': 1, 'nationally': 1, 'globallyrenowned.': 1, 'manufacturing': 1, 'eroded': 1, 'restructuring': 1, 'industry,': 1, 'transitioned': 1, 'into': 1, 'industries.': 1, 'September': 1, '11': 1, 'attacks': 2, '2001': 1, 'destroyed': 1, 'Trade': 1, 'Center,': 1, 'killing': 1, 'almost': 1, '3,000': 1, 'people;': 1, 'they': 1, 'largest': 1, 'terrorist': 1, 'on': 1, 'soil.': 1})\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate ()\n",
    "lines = sc.textFile(\"word_count.txt\")\n",
    "words = lines.flatMap(lambda l:l.split())\n",
    "total_count = words.count()\n",
    "print(total_count)\n",
    "unique_word_counts = words.countByValue()\n",
    "print(unique_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div of numbers is :  1.0\n"
     ]
    }
   ],
   "source": [
    "sampledata = sc.parallelize([70,7,5,2])\n",
    "#aggregate numbers using addition operator\n",
    "div = sampledata.reduce(lambda a,b: a/b)\n",
    "print(\"div of numbers is : \", div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n",
      "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n",
      "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[[1,2,3],[1,2,3],[1,2,3]],\n",
    "    [[1,2,3],[1,2,3],[1,2,3]],\n",
    "    [[1,2,3],[1,2,3],[1,2,3]]\n",
    "    ]\n",
    "nested_rdd = sc.parallelize(arr)\n",
    "#nested_rdd = rdd_arr.flatMap(lambda x: x)\n",
    "flattened_rdd = nested_rdd.flatMap(lambda x: x if isinstance(x, list) else [x])\n",
    "for i in arr:\n",
    "    print(i)\n",
    "#print(rdd_flatten.collect())\n",
    "#dd_flatten2 = rdd_flatten.flatMap(lambda x: x)\n",
    "flattened_rdd.collect()\n",
    "\n",
    "# recursion not spark\n",
    "# def flat_func(arr):\n",
    "#     l = []\n",
    "#     for i in arr:\n",
    "#         if isinstance(i, list):\n",
    "#             l.extend(flat_func(i))\n",
    "#         else:\n",
    "#             l.append(i)\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Chad\n",
    "\n",
    "# arr = [[[1,2,3],[1,2,3],[1,2,3]],\n",
    "#     [[1,2,3],[1,2,3],[1,2,3]],\n",
    "#     [[1,2,3],[1,2,3],[1,2,3]]\n",
    "#     ]\n",
    "\n",
    "# def endless_flatten(nd_list):\n",
    "#     if isinstance(nd_list, list):\n",
    "#         return [item for sublist in nd_list for item in endless_flatten(sublist)]\n",
    "#     else:\n",
    "#         return [nd_list]\n",
    "    \n",
    "# endless_flatten(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('python', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "x = sc.parallelize([(\"spark\", 1), (\"python\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"python\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print (\"Join RDD -> %s\" % (final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)]\n",
    "\n",
    "columns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "# ------Creating data frame Using createDataFrame() function-----\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id    name       city\n",
      "0  1001   Young  Rego Park\n",
      "1  1002   James      Bronx\n",
      "2  1003  Haseeb    Astoria\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+----+------+---------+\n",
      "|  id|  name|     city|\n",
      "+----+------+---------+\n",
      "|1001| Young|Rego Park|\n",
      "|1002| James|    Bronx|\n",
      "|1003|Haseeb|  Astoria|\n",
      "+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "student_dict = {\"id\": [1001, 1002, 1003],\"name\": [ \"Young\", \"James\", \"Haseeb\"],\"city\": [ \"Rego Park\", \"Bronx\", \"Astoria\"]}\n",
    "\n",
    "# --- panda dataframe ----\n",
    "pd_df = pd.DataFrame(student_dict)\n",
    "print(pd_df)\n",
    "\n",
    "\n",
    "# ----- SparkSQL dataframe-----\n",
    "sp_df = spark.createDataFrame(pd_df)\n",
    "sp_df.printSchema()\n",
    "sp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MLS: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Bedrooms: integer (nullable = true)\n",
      " |-- Bathrooms: integer (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Price SQ Ft: double (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "|   MLS|          Location|   Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "|132842|     Arroyo Grande|795000.0|       3|        3|2371|      335.3|Short Sale|\n",
      "|134364|       Paso Robles|399000.0|       4|        3|2818|     141.59|Short Sale|\n",
      "|135141|       Paso Robles|545000.0|       4|        3|3032|     179.75|Short Sale|\n",
      "|135712|         Morro Bay|909000.0|       4|        4|3540|     256.78|Short Sale|\n",
      "|136282|Santa Maria-Orcutt|109900.0|       3|        1|1249|      87.99|Short Sale|\n",
      "|136431|            Oceano|324900.0|       3|        3|1800|      180.5|Short Sale|\n",
      "|137036|Santa Maria-Orcutt|192900.0|       4|        2|1603|     120.34|Short Sale|\n",
      "|137090|Santa Maria-Orcutt|215000.0|       3|        2|1450|     148.28|Short Sale|\n",
      "|137159|         Morro Bay|999000.0|       4|        3|3360|     297.32|Short Sale|\n",
      "|137570|        Atascadero|319000.0|       3|        2|1323|     241.12|Short Sale|\n",
      "|138053|Santa Maria-Orcutt|350000.0|       3|        2|1750|      200.0|Short Sale|\n",
      "|138730|Santa Maria-Orcutt|249000.0|       3|        2|1400|     177.86|Short Sale|\n",
      "|139291|     Arroyo Grande|299000.0|       2|        2|1257|     237.87|Short Sale|\n",
      "|139427|Santa Maria-Orcutt|235900.0|       3|        2|1400|      168.5|Short Sale|\n",
      "|139461|Santa Maria-Orcutt|348000.0|       3|        2|1600|      217.5|Short Sale|\n",
      "|139661|       Paso Robles|314000.0|       4|        3|1794|     175.03|Short Sale|\n",
      "|139918|        Los Alamos|399000.0|       4|        2|1850|     215.68|Short Sale|\n",
      "|139932|        San Miguel|599000.0|       3|        3|2950|     203.05|Short Sale|\n",
      "|140044|       Paso Robles|299000.0|       3|        2|1719|     173.94|Short Sale|\n",
      "|140073|   San Luis Obispo|425000.0|       3|        3|1472|     288.72|Short Sale|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Sparkapplicationdemo').getOrCreate()\n",
    "#used .load() here because of how the data is structured. \n",
    "#pyspark can infer the schema cause its csv, just need to do format=\"csv\"\n",
    "#without header=True we'd throw away our data types\n",
    "df = spark.read.load(\"RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               null|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         null|           1|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               null|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         null|           2|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               null|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         null|       61391|   TX|           null|      null|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         null|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         null|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               null|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         null|           4|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         null|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         null|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               null|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         null|       49346|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         null|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               null|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         null|       49348|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               null|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         null|           3|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               null|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         null|       54354|   AL|           null|      null|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         null|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         null|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         null|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         null|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"zipcode.json\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test SQL app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06|  2003-01-13| 2003-01-10|pending|                null|           363|\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10|  2003-01-18| 2003-01-14|pending|                null|           181|\n",
      "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
      "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
      "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
      "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
      "|      10107|2003-02-24|  2003-03-03| 2003-02-26|pending|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
      "|      10109|2003-03-10|  2003-03-19| 2003-03-11|pending|Customer requeste...|           486|\n",
      "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
      "|      10111|2003-03-25|  2003-03-31| 2003-03-30|pending|                null|           129|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26|  2003-04-02| 2003-03-27|pending|                null|           124|\n",
      "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
      "|      10115|2003-04-04|  2003-04-12| 2003-04-07|pending|                null|           424|\n",
      "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
      "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
      "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     password=\"password\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=\"classicmodels.orders\").load()\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Anyone having trouble....if you want to try to connect to my database....\n",
    "\n",
    "# Michael Schmidt 12:20 PM\n",
    "# df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "#                                      user=\"michaelwschmidt_user\",\\\n",
    "#                                      password=\"password\",\\\n",
    "#                                      url=\"jdbc:mysql://mikey.helioho.st:3306/michaelwschmidt_classicmodels\",\\\n",
    "#                                      dbtable=\"orders\").load()\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType, DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"orderDate\",StringType(),True), \\\n",
    "    StructField(\"comments\",StringType(),True), \\\n",
    "    StructField(\"customerNumber\",IntegerType(),True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06|  2003-01-13| 2003-01-10|pending|                null|           363|\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10|  2003-01-18| 2003-01-14|pending|                null|           181|\n",
      "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
      "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
      "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
      "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
      "|      10107|2003-02-24|  2003-03-03| 2003-02-26|pending|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
      "|      10109|2003-03-10|  2003-03-19| 2003-03-11|pending|Customer requeste...|           486|\n",
      "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
      "|      10111|2003-03-25|  2003-03-31| 2003-03-30|pending|                null|           129|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26|  2003-04-02| 2003-03-27|pending|                null|           124|\n",
      "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
      "|      10115|2003-04-04|  2003-04-12| 2003-04-07|pending|                null|           424|\n",
      "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
      "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
      "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
